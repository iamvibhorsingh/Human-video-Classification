{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mohanrajmit/Human-Action-Classification-/blob/master/Action%20Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IvXjAHsbcKY7"
   },
   "source": [
    "# Activity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zOHQ7a5_aqUD",
    "outputId": "361af30e-ce20-450c-bcc3-a22a002fd5f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nidhi\\anaconda3.5\\envs\\gan\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Input, LSTM, Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard,EarlyStopping\n",
    "from keras.utils.io_utils import HDF5Matrix\n",
    "\n",
    "SEQ_LEN = 30\n",
    "MAX_SEQ_LEN = 200\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "XgxT_-jFB-kG",
    "outputId": "a3bc68f5-b7a0-4795-f4b6-f6d504a97291"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Human-Action-Classification-'...\n",
      "remote: Enumerating objects: 1269, done.\u001b[K\n",
      "remote: Counting objects:   0% (1/1269)   \u001b[K\r",
      "remote: Counting objects:   1% (13/1269)   \u001b[K\r",
      "remote: Counting objects:   2% (26/1269)   \u001b[K\r",
      "remote: Counting objects:   3% (39/1269)   \u001b[K\r",
      "remote: Counting objects:   4% (51/1269)   \u001b[K\r",
      "remote: Counting objects:   5% (64/1269)   \u001b[K\r",
      "remote: Counting objects:   6% (77/1269)   \u001b[K\r",
      "remote: Counting objects:   7% (89/1269)   \u001b[K\r",
      "remote: Counting objects:   8% (102/1269)   \u001b[K\r",
      "remote: Counting objects:   9% (115/1269)   \u001b[K\r",
      "remote: Counting objects:  10% (127/1269)   \u001b[K\r",
      "remote: Counting objects:  11% (140/1269)   \u001b[K\r",
      "remote: Counting objects:  12% (153/1269)   \u001b[K\r",
      "remote: Counting objects:  13% (165/1269)   \u001b[K\r",
      "remote: Counting objects:  14% (178/1269)   \u001b[K\r",
      "remote: Counting objects:  15% (191/1269)   \u001b[K\r",
      "remote: Counting objects:  16% (204/1269)   \u001b[K\r",
      "remote: Counting objects:  17% (216/1269)   \u001b[K\r",
      "remote: Counting objects:  18% (229/1269)   \u001b[K\r",
      "remote: Counting objects:  19% (242/1269)   \u001b[K\r",
      "remote: Counting objects:  20% (254/1269)   \u001b[K\r",
      "remote: Counting objects:  21% (267/1269)   \u001b[K\r",
      "remote: Counting objects:  22% (280/1269)   \u001b[K\r",
      "remote: Counting objects:  23% (292/1269)   \u001b[K\r",
      "remote: Counting objects:  24% (305/1269)   \u001b[K\r",
      "remote: Counting objects:  25% (318/1269)   \u001b[K\r",
      "remote: Counting objects:  26% (330/1269)   \u001b[K\r",
      "remote: Counting objects:  27% (343/1269)   \u001b[K\r",
      "remote: Counting objects:  28% (356/1269)   \u001b[K\r",
      "remote: Counting objects:  29% (369/1269)   \u001b[K\r",
      "remote: Counting objects:  30% (381/1269)   \u001b[K\r",
      "remote: Counting objects:  31% (394/1269)   \u001b[K\r",
      "remote: Counting objects:  32% (407/1269)   \u001b[K\r",
      "remote: Counting objects:  33% (419/1269)   \u001b[K\r",
      "remote: Counting objects:  34% (432/1269)   \u001b[K\r",
      "remote: Counting objects:  35% (445/1269)   \u001b[K\r",
      "remote: Counting objects:  36% (457/1269)   \u001b[K\r",
      "remote: Counting objects:  37% (470/1269)   \u001b[K\r",
      "remote: Counting objects:  38% (483/1269)   \u001b[K\r",
      "remote: Counting objects:  39% (495/1269)   \u001b[K\r",
      "remote: Counting objects:  40% (508/1269)   \u001b[K\r",
      "remote: Counting objects:  41% (521/1269)   \u001b[K\r",
      "remote: Counting objects:  42% (533/1269)   \u001b[K\r",
      "remote: Counting objects:  43% (546/1269)   \u001b[K\r",
      "remote: Counting objects:  44% (559/1269)   \u001b[K\r",
      "remote: Counting objects:  45% (572/1269)   \u001b[K\r",
      "remote: Counting objects:  46% (584/1269)   \u001b[K\r",
      "remote: Counting objects:  47% (597/1269)   \u001b[K\r",
      "remote: Counting objects:  48% (610/1269)   \u001b[K\r",
      "remote: Counting objects:  49% (622/1269)   \u001b[K\r",
      "remote: Counting objects:  50% (635/1269)   \u001b[K\r",
      "remote: Counting objects:  51% (648/1269)   \u001b[K\r",
      "remote: Counting objects:  52% (660/1269)   \u001b[K\r",
      "remote: Counting objects:  53% (673/1269)   \u001b[K\r",
      "remote: Counting objects:  54% (686/1269)   \u001b[K\r",
      "remote: Counting objects:  55% (698/1269)   \u001b[K\r",
      "remote: Counting objects:  56% (711/1269)   \u001b[K\r",
      "remote: Counting objects:  57% (724/1269)   \u001b[K\r",
      "remote: Counting objects:  58% (737/1269)   \u001b[K\r",
      "remote: Counting objects:  59% (749/1269)   \u001b[K\r",
      "remote: Counting objects:  60% (762/1269)   \u001b[K\r",
      "remote: Counting objects:  61% (775/1269)   \u001b[K\r",
      "remote: Counting objects:  62% (787/1269)   \u001b[K\r",
      "remote: Counting objects:  63% (800/1269)   \u001b[K\r",
      "remote: Counting objects:  64% (813/1269)   \u001b[K\r",
      "remote: Counting objects:  65% (825/1269)   \u001b[K\r",
      "remote: Counting objects:  66% (838/1269)   \u001b[K\r",
      "remote: Counting objects:  67% (851/1269)   \u001b[K\r",
      "remote: Counting objects:  68% (863/1269)   \u001b[K\r",
      "remote: Counting objects:  69% (876/1269)   \u001b[K\r",
      "remote: Counting objects:  70% (889/1269)   \u001b[K\r",
      "remote: Counting objects:  71% (901/1269)   \u001b[K\r",
      "remote: Counting objects:  72% (914/1269)   \u001b[K\r",
      "remote: Counting objects:  73% (927/1269)   \u001b[K\r",
      "remote: Counting objects:  74% (940/1269)   \u001b[K\r",
      "remote: Counting objects:  75% (952/1269)   \u001b[K\r",
      "remote: Counting objects:  76% (965/1269)   \u001b[K\r",
      "remote: Counting objects:  77% (978/1269)   \u001b[K\r",
      "remote: Counting objects:  78% (990/1269)   \u001b[K\r",
      "remote: Counting objects:  79% (1003/1269)   \u001b[K\r",
      "remote: Counting objects:  80% (1016/1269)   \u001b[K\r",
      "remote: Counting objects:  81% (1028/1269)   \u001b[K\r",
      "remote: Counting objects:  82% (1041/1269)   \u001b[K\r",
      "remote: Counting objects:  83% (1054/1269)   \u001b[K\r",
      "remote: Counting objects:  84% (1066/1269)   \u001b[K\r",
      "remote: Counting objects:  85% (1079/1269)   \u001b[K\r",
      "remote: Counting objects:  86% (1092/1269)   \u001b[K\r",
      "remote: Counting objects:  87% (1105/1269)   \u001b[K\r",
      "remote: Counting objects:  88% (1117/1269)   \u001b[K\r",
      "remote: Counting objects:  89% (1130/1269)   \u001b[K\r",
      "remote: Counting objects:  90% (1143/1269)   \u001b[K\r",
      "remote: Counting objects:  91% (1155/1269)   \u001b[K\r",
      "remote: Counting objects:  92% (1168/1269)   \u001b[K\r",
      "remote: Counting objects:  93% (1181/1269)   \u001b[K\r",
      "remote: Counting objects:  94% (1193/1269)   \u001b[K\r",
      "remote: Counting objects:  95% (1206/1269)   \u001b[K\r",
      "remote: Counting objects:  96% (1219/1269)   \u001b[K\r",
      "remote: Counting objects:  97% (1231/1269)   \u001b[K\r",
      "remote: Counting objects:  98% (1244/1269)   \u001b[K\r",
      "remote: Counting objects:  99% (1257/1269)   \u001b[K\r",
      "remote: Counting objects: 100% (1269/1269)   \u001b[K\r",
      "remote: Counting objects: 100% (1269/1269), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1264/1264), done.\u001b[K\n",
      "remote: Total 1269 (delta 18), reused 1236 (delta 2), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (1269/1269), 469.58 MiB | 30.72 MiB/s, done.\n",
      "Resolving deltas: 100% (18/18), done.\n",
      "Checking out files: 100% (1218/1218), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mohanrajmit/Human-Action-Classification-.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "doq8RcCGCFNx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8rQW1WYscPY7"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Get model with pretrained weights.\n",
    "    base_model = InceptionV3(\n",
    "    weights='imagenet',\n",
    "    include_top=True)\n",
    "    \n",
    "    \n",
    "    # We'll extract features at the final pool layer.\n",
    "    model = Model(\n",
    "        inputs=base_model.input,\n",
    "        outputs=base_model.get_layer('avg_pool').output)\n",
    "    \n",
    "    # Getting the data\n",
    "    df = get_data('Human-Action-Classification--master/data/data_file.csv')\n",
    "    \n",
    "    # Clean the data\n",
    "    df_clean = clean_data(df)\n",
    "    \n",
    "    # Creating index-label maps and inverse_maps\n",
    "    label_index, index_label = get_class_dict(df_clean)\n",
    "    \n",
    "    # Split the dataset into train and test\n",
    "    train, test = split_train_test(df_clean)\n",
    "    \n",
    "    # Encoding the dataset\n",
    "    encode_dataset(train, model, label_index, \"train\")\n",
    "    encode_dataset(test,model,label_index,\"test\")\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6CSH8rX1IKYD",
    "outputId": "541db8ba-1599-4f76-b000-90cd5d5b7548"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M9-tCCZscPW6"
   },
   "outputs": [],
   "source": [
    "def get_data(path, if_pd=False):\n",
    "    \"\"\"Load our data from file.\"\"\"\n",
    "    names = ['partition', 'class', 'video_name', 'frames']\n",
    "    df = pd.read_csv(path,names=names)\n",
    "    return df\n",
    "\n",
    "def get_class_dict(df):\n",
    "    class_name =  list(df['class'].unique())\n",
    "    index = np.arange(0, len(class_name))\n",
    "    label_index = dict(zip(class_name, index))\n",
    "    index_label = dict(zip(index, class_name))\n",
    "    return (label_index, index_label)\n",
    "    \n",
    "def clean_data(df):\n",
    "    mask = np.logical_and(df['frames'] >= SEQ_LEN, df['frames'] <= MAX_SEQ_LEN)\n",
    "    df = df[mask]\n",
    "    return df\n",
    "def split_train_test(df):\n",
    "    partition =  (df.groupby(['partition']))\n",
    "    un = df['partition'].unique()\n",
    "    train = partition.get_group(un[0])\n",
    "    test = partition.get_group(un[1])\n",
    "    return (train, test)\n",
    "\n",
    "def preprocess_image(img):\n",
    "    img = cv2.resize(img, (299,299))\n",
    "    return preprocess_input(img)\n",
    "    \n",
    "    \n",
    "def encode_video(row, model, label_index):\n",
    "    cap = cv2.VideoCapture(os.path.join(\"Human-Action-Classification--master/data\",\"UCF-101\",str(row[\"class\"].iloc[0]) ,str(row[\"video_name\"].iloc[0]) + \".avi\"))\n",
    "    images = []  \n",
    "    for i in range(SEQ_LEN):\n",
    "        ret, frame = cap.read()\n",
    "        frame = preprocess_image(frame)\n",
    "        images.append(frame)\n",
    "    \n",
    "    \n",
    "    features = model.predict(np.array(images))\n",
    "    index = label_index[row[\"class\"].iloc[0]]\n",
    "    print(index)\n",
    "    #y_onehot = to_categorical(index, len(label_index.keys()))\n",
    "    \n",
    "    return features, index\n",
    "\n",
    "from keras.utils import np_utils\n",
    "def encode_dataset(data, model, label_index, phase):\n",
    "    input_f = []\n",
    "    output_y = []\n",
    "    required_classes = [\"ApplyEyeMakeup\" , \"ApplyLipstick\" , \"Archery\" , \"BabyCrawling\" , \"BalanceBeam\" ,\n",
    "                       \"BandMarching\" , \"BaseballPitch\" , \"Basketball\" , \"BasketballDunk\"]\n",
    "   \n",
    "    \n",
    "    for i in tqdm(range(data.shape[0])):\n",
    "    # Check whether the given row , is of a class that is required\n",
    "        if str(data.iloc[[i]][\"class\"].iloc[0]) in required_classes:\n",
    " \n",
    "            features,y =  encode_video(data.iloc[[i]], model, label_index)\n",
    "            input_f.append(features)\n",
    "            output_y.append(y)\n",
    "        \n",
    "    \n",
    "    le_labels = np_utils.to_categorical(output_y)\n",
    "    f = h5py.File(phase+'_8'+'.h5', 'w')\n",
    "    f.create_dataset(phase, data=np.array(input_f))\n",
    "    f.create_dataset(phase+\"_labels\", data=np.array(le_labels))\n",
    "    \n",
    "    del input_f[:]\n",
    "    del output_y[:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 16837
    },
    "colab_type": "code",
    "id": "NnkJp3ljcPam",
    "outputId": "50afd4a6-4379-43e0-c632-c4d4e74b674f"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'Human-Action-Classification--master/data/data_file.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-09c69e9405cc>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Getting the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Human-Action-Classification--master/data/data_file.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Clean the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-c3eca9a39c84>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(path, if_pd)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\"Load our data from file.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'partition'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'class'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'video_name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'frames'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nidhi\\anaconda3.5\\envs\\gan\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nidhi\\anaconda3.5\\envs\\gan\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nidhi\\anaconda3.5\\envs\\gan\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nidhi\\anaconda3.5\\envs\\gan\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nidhi\\anaconda3.5\\envs\\gan\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'Human-Action-Classification--master/data/data_file.csv' does not exist"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lGQuJIzbf1tF"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S0uJMq0tViq3"
   },
   "outputs": [],
   "source": [
    "def lstm():\n",
    "    \"\"\"Build a simple LSTM network. We pass the extracted features from\n",
    "    our CNN to this model predominantly.\"\"\"\n",
    "    input_shape = (SEQ_LEN, 2048)\n",
    "    # Model.\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(2048,input_shape=input_shape,dropout=0.5))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(9, activation='softmax'))\n",
    "    #model.add(Dense(10, activation='softmax'))\"\"\"\n",
    "    checkpoint = ModelCheckpoint(filepath='models\\\\checkpoint-{epoch:02d}-{val_loss:.2f}.hdf5')\n",
    "    \n",
    "    tb_callback = TensorBoard(\n",
    "    log_dir=\"logs\",\n",
    "    histogram_freq=2,\n",
    "    write_graph=True\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor = 'val_loss',patience= 10)\n",
    "    \n",
    "    callback_list = [checkpoint, tb_callback]\n",
    "\n",
    "    optimizer = Adam(lr=1e-5, decay=1e-6)\n",
    "    metrics = ['accuracy', 'top_k_categorical_accuracy']\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=metrics)\n",
    "    return model, callback_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3598
    },
    "colab_type": "code",
    "id": "kJXchpTpIxdN",
    "outputId": "cdb0d2d1-bb58-49e7-b185-21c76ec3fe43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(703, 30, 2048)\n",
      "(703, 9)\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "(290, 30, 2048)\n",
      "(290, 9)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 703 samples, validate on 290 samples\n",
      "Epoch 1/100\n",
      " - 22s - loss: 2.5163 - acc: 0.1038 - top_k_categorical_accuracy: 0.5164 - val_loss: 2.2378 - val_acc: 0.1448 - val_top_k_categorical_accuracy: 0.5793\n",
      "Epoch 2/100\n",
      " - 18s - loss: 2.4083 - acc: 0.1138 - top_k_categorical_accuracy: 0.5804 - val_loss: 2.1957 - val_acc: 0.1862 - val_top_k_categorical_accuracy: 0.6345\n",
      "Epoch 3/100\n",
      " - 18s - loss: 2.3939 - acc: 0.1266 - top_k_categorical_accuracy: 0.5917 - val_loss: 2.1551 - val_acc: 0.2276 - val_top_k_categorical_accuracy: 0.6448\n",
      "Epoch 4/100\n",
      " - 18s - loss: 2.3462 - acc: 0.1294 - top_k_categorical_accuracy: 0.5875 - val_loss: 2.1358 - val_acc: 0.2310 - val_top_k_categorical_accuracy: 0.6379\n",
      "Epoch 5/100\n",
      " - 18s - loss: 2.3219 - acc: 0.1508 - top_k_categorical_accuracy: 0.6102 - val_loss: 2.1092 - val_acc: 0.2345 - val_top_k_categorical_accuracy: 0.7000\n",
      "Epoch 6/100\n",
      " - 18s - loss: 2.3143 - acc: 0.1550 - top_k_categorical_accuracy: 0.5989 - val_loss: 2.0715 - val_acc: 0.2828 - val_top_k_categorical_accuracy: 0.7000\n",
      "Epoch 7/100\n",
      " - 18s - loss: 2.2710 - acc: 0.1536 - top_k_categorical_accuracy: 0.6273 - val_loss: 2.0603 - val_acc: 0.2793 - val_top_k_categorical_accuracy: 0.7000\n",
      "Epoch 8/100\n",
      " - 18s - loss: 2.2742 - acc: 0.1693 - top_k_categorical_accuracy: 0.6344 - val_loss: 2.0502 - val_acc: 0.2759 - val_top_k_categorical_accuracy: 0.6966\n",
      "Epoch 9/100\n",
      " - 18s - loss: 2.3161 - acc: 0.1280 - top_k_categorical_accuracy: 0.6003 - val_loss: 2.0380 - val_acc: 0.3034 - val_top_k_categorical_accuracy: 0.7345\n",
      "Epoch 10/100\n",
      " - 18s - loss: 2.2267 - acc: 0.1579 - top_k_categorical_accuracy: 0.6572 - val_loss: 2.0347 - val_acc: 0.2862 - val_top_k_categorical_accuracy: 0.7345\n",
      "Epoch 11/100\n",
      " - 18s - loss: 2.2588 - acc: 0.1792 - top_k_categorical_accuracy: 0.6216 - val_loss: 2.0216 - val_acc: 0.2862 - val_top_k_categorical_accuracy: 0.7172\n",
      "Epoch 12/100\n",
      " - 18s - loss: 2.2459 - acc: 0.1479 - top_k_categorical_accuracy: 0.6657 - val_loss: 2.0018 - val_acc: 0.2897 - val_top_k_categorical_accuracy: 0.7379\n",
      "Epoch 13/100\n",
      " - 18s - loss: 2.2151 - acc: 0.1863 - top_k_categorical_accuracy: 0.6643 - val_loss: 1.9874 - val_acc: 0.2690 - val_top_k_categorical_accuracy: 0.7483\n",
      "Epoch 14/100\n",
      " - 18s - loss: 2.1809 - acc: 0.1792 - top_k_categorical_accuracy: 0.7027 - val_loss: 1.9774 - val_acc: 0.2793 - val_top_k_categorical_accuracy: 0.7724\n",
      "Epoch 15/100\n",
      " - 18s - loss: 2.1877 - acc: 0.1878 - top_k_categorical_accuracy: 0.6942 - val_loss: 1.9720 - val_acc: 0.2793 - val_top_k_categorical_accuracy: 0.7483\n",
      "Epoch 16/100\n",
      " - 18s - loss: 2.1826 - acc: 0.1878 - top_k_categorical_accuracy: 0.6700 - val_loss: 1.9647 - val_acc: 0.2828 - val_top_k_categorical_accuracy: 0.7586\n",
      "Epoch 17/100\n",
      " - 18s - loss: 2.1561 - acc: 0.2176 - top_k_categorical_accuracy: 0.6572 - val_loss: 1.9504 - val_acc: 0.3000 - val_top_k_categorical_accuracy: 0.7621\n",
      "Epoch 18/100\n",
      " - 17s - loss: 2.1289 - acc: 0.2020 - top_k_categorical_accuracy: 0.7155 - val_loss: 1.9384 - val_acc: 0.3207 - val_top_k_categorical_accuracy: 0.7931\n",
      "Epoch 19/100\n",
      " - 18s - loss: 2.1655 - acc: 0.2020 - top_k_categorical_accuracy: 0.6885 - val_loss: 1.9202 - val_acc: 0.3207 - val_top_k_categorical_accuracy: 0.7897\n",
      "Epoch 20/100\n",
      " - 18s - loss: 2.1200 - acc: 0.2105 - top_k_categorical_accuracy: 0.7141 - val_loss: 1.9081 - val_acc: 0.3241 - val_top_k_categorical_accuracy: 0.8172\n",
      "Epoch 21/100\n",
      " - 18s - loss: 2.1198 - acc: 0.2162 - top_k_categorical_accuracy: 0.6999 - val_loss: 1.9128 - val_acc: 0.3310 - val_top_k_categorical_accuracy: 0.8103\n",
      "Epoch 22/100\n",
      " - 18s - loss: 2.0938 - acc: 0.2034 - top_k_categorical_accuracy: 0.7240 - val_loss: 1.8998 - val_acc: 0.3207 - val_top_k_categorical_accuracy: 0.8414\n",
      "Epoch 23/100\n",
      " - 18s - loss: 2.0751 - acc: 0.2376 - top_k_categorical_accuracy: 0.7141 - val_loss: 1.8922 - val_acc: 0.3276 - val_top_k_categorical_accuracy: 0.8310\n",
      "Epoch 24/100\n",
      " - 18s - loss: 2.1114 - acc: 0.2276 - top_k_categorical_accuracy: 0.6942 - val_loss: 1.8753 - val_acc: 0.3414 - val_top_k_categorical_accuracy: 0.8379\n",
      "Epoch 25/100\n",
      " - 18s - loss: 2.0914 - acc: 0.2361 - top_k_categorical_accuracy: 0.7141 - val_loss: 1.8654 - val_acc: 0.3276 - val_top_k_categorical_accuracy: 0.8414\n",
      "Epoch 26/100\n",
      " - 18s - loss: 2.0952 - acc: 0.2347 - top_k_categorical_accuracy: 0.7255 - val_loss: 1.8725 - val_acc: 0.3207 - val_top_k_categorical_accuracy: 0.8345\n",
      "Epoch 27/100\n",
      " - 18s - loss: 2.0897 - acc: 0.2176 - top_k_categorical_accuracy: 0.7297 - val_loss: 1.8672 - val_acc: 0.3172 - val_top_k_categorical_accuracy: 0.8517\n",
      "Epoch 28/100\n",
      " - 18s - loss: 2.0752 - acc: 0.2461 - top_k_categorical_accuracy: 0.7326 - val_loss: 1.8532 - val_acc: 0.3310 - val_top_k_categorical_accuracy: 0.8483\n",
      "Epoch 29/100\n",
      " - 18s - loss: 2.0752 - acc: 0.2319 - top_k_categorical_accuracy: 0.7269 - val_loss: 1.8524 - val_acc: 0.3241 - val_top_k_categorical_accuracy: 0.8172\n",
      "Epoch 30/100\n",
      " - 18s - loss: 2.0414 - acc: 0.2532 - top_k_categorical_accuracy: 0.7496 - val_loss: 1.8439 - val_acc: 0.3483 - val_top_k_categorical_accuracy: 0.8207\n",
      "Epoch 31/100\n",
      " - 18s - loss: 2.0537 - acc: 0.2361 - top_k_categorical_accuracy: 0.7354 - val_loss: 1.8385 - val_acc: 0.3414 - val_top_k_categorical_accuracy: 0.7897\n",
      "Epoch 32/100\n",
      " - 18s - loss: 2.0134 - acc: 0.2518 - top_k_categorical_accuracy: 0.7824 - val_loss: 1.8285 - val_acc: 0.3379 - val_top_k_categorical_accuracy: 0.8276\n",
      "Epoch 33/100\n",
      " - 18s - loss: 1.9937 - acc: 0.2774 - top_k_categorical_accuracy: 0.7639 - val_loss: 1.8286 - val_acc: 0.3414 - val_top_k_categorical_accuracy: 0.8276\n",
      "Epoch 34/100\n",
      " - 18s - loss: 2.0096 - acc: 0.2546 - top_k_categorical_accuracy: 0.7639 - val_loss: 1.8195 - val_acc: 0.3345 - val_top_k_categorical_accuracy: 0.8310\n",
      "Epoch 35/100\n",
      " - 18s - loss: 1.9845 - acc: 0.2788 - top_k_categorical_accuracy: 0.7568 - val_loss: 1.8188 - val_acc: 0.3414 - val_top_k_categorical_accuracy: 0.8310\n",
      "Epoch 36/100\n",
      " - 18s - loss: 1.9717 - acc: 0.2817 - top_k_categorical_accuracy: 0.7852 - val_loss: 1.8135 - val_acc: 0.3517 - val_top_k_categorical_accuracy: 0.8276\n",
      "Epoch 37/100\n",
      " - 18s - loss: 1.9596 - acc: 0.2760 - top_k_categorical_accuracy: 0.7866 - val_loss: 1.8092 - val_acc: 0.3276 - val_top_k_categorical_accuracy: 0.8103\n",
      "Epoch 38/100\n",
      " - 18s - loss: 1.9538 - acc: 0.2817 - top_k_categorical_accuracy: 0.7752 - val_loss: 1.7956 - val_acc: 0.3310 - val_top_k_categorical_accuracy: 0.8310\n",
      "Epoch 39/100\n",
      " - 18s - loss: 1.9344 - acc: 0.3058 - top_k_categorical_accuracy: 0.7937 - val_loss: 1.7859 - val_acc: 0.3414 - val_top_k_categorical_accuracy: 0.8207\n",
      "Epoch 40/100\n",
      " - 18s - loss: 1.9834 - acc: 0.2731 - top_k_categorical_accuracy: 0.7824 - val_loss: 1.7879 - val_acc: 0.3379 - val_top_k_categorical_accuracy: 0.8310\n",
      "Epoch 41/100\n",
      " - 18s - loss: 1.9469 - acc: 0.3229 - top_k_categorical_accuracy: 0.7937 - val_loss: 1.7807 - val_acc: 0.3379 - val_top_k_categorical_accuracy: 0.8241\n",
      "Epoch 42/100\n",
      " - 18s - loss: 1.9579 - acc: 0.2902 - top_k_categorical_accuracy: 0.7966 - val_loss: 1.7743 - val_acc: 0.3483 - val_top_k_categorical_accuracy: 0.8414\n",
      "Epoch 43/100\n",
      " - 18s - loss: 1.9295 - acc: 0.3144 - top_k_categorical_accuracy: 0.7881 - val_loss: 1.7676 - val_acc: 0.3483 - val_top_k_categorical_accuracy: 0.8552\n",
      "Epoch 44/100\n",
      " - 18s - loss: 1.9049 - acc: 0.3001 - top_k_categorical_accuracy: 0.8193 - val_loss: 1.7654 - val_acc: 0.3448 - val_top_k_categorical_accuracy: 0.8448\n",
      "Epoch 45/100\n",
      " - 18s - loss: 1.9463 - acc: 0.3129 - top_k_categorical_accuracy: 0.7710 - val_loss: 1.7442 - val_acc: 0.3586 - val_top_k_categorical_accuracy: 0.8759\n",
      "Epoch 46/100\n",
      " - 18s - loss: 1.8878 - acc: 0.3357 - top_k_categorical_accuracy: 0.8137 - val_loss: 1.7489 - val_acc: 0.3448 - val_top_k_categorical_accuracy: 0.8517\n",
      "Epoch 47/100\n",
      " - 18s - loss: 1.9229 - acc: 0.2802 - top_k_categorical_accuracy: 0.8051 - val_loss: 1.7555 - val_acc: 0.3552 - val_top_k_categorical_accuracy: 0.8655\n",
      "Epoch 48/100\n",
      " - 18s - loss: 1.8938 - acc: 0.3101 - top_k_categorical_accuracy: 0.8094 - val_loss: 1.7294 - val_acc: 0.3655 - val_top_k_categorical_accuracy: 0.8793\n",
      "Epoch 49/100\n",
      " - 18s - loss: 1.8587 - acc: 0.3428 - top_k_categorical_accuracy: 0.8151 - val_loss: 1.7293 - val_acc: 0.3483 - val_top_k_categorical_accuracy: 0.8931\n",
      "Epoch 50/100\n",
      " - 18s - loss: 1.8703 - acc: 0.3243 - top_k_categorical_accuracy: 0.8293 - val_loss: 1.7094 - val_acc: 0.3552 - val_top_k_categorical_accuracy: 0.9000\n",
      "Epoch 51/100\n",
      " - 18s - loss: 1.8958 - acc: 0.3172 - top_k_categorical_accuracy: 0.8080 - val_loss: 1.7077 - val_acc: 0.3448 - val_top_k_categorical_accuracy: 0.9000\n",
      "Epoch 52/100\n",
      " - 18s - loss: 1.8754 - acc: 0.3186 - top_k_categorical_accuracy: 0.8336 - val_loss: 1.7034 - val_acc: 0.3621 - val_top_k_categorical_accuracy: 0.9000\n",
      "Epoch 53/100\n",
      " - 18s - loss: 1.8663 - acc: 0.3300 - top_k_categorical_accuracy: 0.8393 - val_loss: 1.7050 - val_acc: 0.3552 - val_top_k_categorical_accuracy: 0.9000\n",
      "Epoch 54/100\n",
      " - 18s - loss: 1.8639 - acc: 0.3385 - top_k_categorical_accuracy: 0.8179 - val_loss: 1.6994 - val_acc: 0.3655 - val_top_k_categorical_accuracy: 0.8966\n",
      "Epoch 55/100\n",
      " - 18s - loss: 1.8358 - acc: 0.3186 - top_k_categorical_accuracy: 0.8549 - val_loss: 1.6968 - val_acc: 0.3690 - val_top_k_categorical_accuracy: 0.8862\n",
      "Epoch 56/100\n",
      " - 18s - loss: 1.8046 - acc: 0.3613 - top_k_categorical_accuracy: 0.8364 - val_loss: 1.7045 - val_acc: 0.3517 - val_top_k_categorical_accuracy: 0.8966\n",
      "Epoch 57/100\n",
      " - 18s - loss: 1.8088 - acc: 0.3599 - top_k_categorical_accuracy: 0.8435 - val_loss: 1.6840 - val_acc: 0.3828 - val_top_k_categorical_accuracy: 0.9138\n",
      "Epoch 58/100\n",
      " - 18s - loss: 1.8533 - acc: 0.3357 - top_k_categorical_accuracy: 0.8407 - val_loss: 1.6868 - val_acc: 0.3517 - val_top_k_categorical_accuracy: 0.9000\n",
      "Epoch 59/100\n",
      " - 18s - loss: 1.8019 - acc: 0.3428 - top_k_categorical_accuracy: 0.8393 - val_loss: 1.6841 - val_acc: 0.3552 - val_top_k_categorical_accuracy: 0.9034\n",
      "Epoch 60/100\n",
      " - 18s - loss: 1.8309 - acc: 0.3371 - top_k_categorical_accuracy: 0.8364 - val_loss: 1.6990 - val_acc: 0.3345 - val_top_k_categorical_accuracy: 0.8931\n",
      "Epoch 61/100\n",
      " - 18s - loss: 1.8318 - acc: 0.3371 - top_k_categorical_accuracy: 0.8236 - val_loss: 1.6878 - val_acc: 0.3586 - val_top_k_categorical_accuracy: 0.8931\n",
      "Epoch 62/100\n",
      " - 18s - loss: 1.7889 - acc: 0.3599 - top_k_categorical_accuracy: 0.8421 - val_loss: 1.6911 - val_acc: 0.3517 - val_top_k_categorical_accuracy: 0.8966\n",
      "Epoch 63/100\n",
      " - 18s - loss: 1.7880 - acc: 0.3741 - top_k_categorical_accuracy: 0.8435 - val_loss: 1.6973 - val_acc: 0.3586 - val_top_k_categorical_accuracy: 0.8966\n",
      "Epoch 64/100\n",
      " - 18s - loss: 1.7645 - acc: 0.3556 - top_k_categorical_accuracy: 0.8634 - val_loss: 1.6881 - val_acc: 0.3690 - val_top_k_categorical_accuracy: 0.9103\n",
      "Epoch 65/100\n",
      " - 18s - loss: 1.8018 - acc: 0.3485 - top_k_categorical_accuracy: 0.8450 - val_loss: 1.6898 - val_acc: 0.3448 - val_top_k_categorical_accuracy: 0.9034\n",
      "Epoch 66/100\n",
      " - 18s - loss: 1.7766 - acc: 0.3528 - top_k_categorical_accuracy: 0.8478 - val_loss: 1.6797 - val_acc: 0.3552 - val_top_k_categorical_accuracy: 0.9207\n",
      "Epoch 67/100\n",
      " - 18s - loss: 1.7788 - acc: 0.3457 - top_k_categorical_accuracy: 0.8549 - val_loss: 1.6787 - val_acc: 0.3724 - val_top_k_categorical_accuracy: 0.9414\n",
      "Epoch 68/100\n",
      " - 18s - loss: 1.7335 - acc: 0.3841 - top_k_categorical_accuracy: 0.8535 - val_loss: 1.6742 - val_acc: 0.3621 - val_top_k_categorical_accuracy: 0.9207\n",
      "Epoch 69/100\n",
      " - 18s - loss: 1.7442 - acc: 0.3855 - top_k_categorical_accuracy: 0.8421 - val_loss: 1.6849 - val_acc: 0.3690 - val_top_k_categorical_accuracy: 0.9310\n",
      "Epoch 70/100\n",
      " - 18s - loss: 1.7625 - acc: 0.3755 - top_k_categorical_accuracy: 0.8435 - val_loss: 1.6536 - val_acc: 0.3690 - val_top_k_categorical_accuracy: 0.9448\n",
      "Epoch 71/100\n",
      " - 18s - loss: 1.7508 - acc: 0.3556 - top_k_categorical_accuracy: 0.8578 - val_loss: 1.6525 - val_acc: 0.3793 - val_top_k_categorical_accuracy: 0.9379\n",
      "Epoch 72/100\n",
      " - 18s - loss: 1.7734 - acc: 0.3642 - top_k_categorical_accuracy: 0.8307 - val_loss: 1.6502 - val_acc: 0.3759 - val_top_k_categorical_accuracy: 0.9345\n",
      "Epoch 73/100\n",
      " - 18s - loss: 1.7250 - acc: 0.3670 - top_k_categorical_accuracy: 0.8634 - val_loss: 1.6322 - val_acc: 0.3621 - val_top_k_categorical_accuracy: 0.9276\n",
      "Epoch 74/100\n",
      " - 18s - loss: 1.7397 - acc: 0.3869 - top_k_categorical_accuracy: 0.8478 - val_loss: 1.6395 - val_acc: 0.3897 - val_top_k_categorical_accuracy: 0.9207\n",
      "Epoch 75/100\n",
      " - 18s - loss: 1.7079 - acc: 0.3841 - top_k_categorical_accuracy: 0.8762 - val_loss: 1.6375 - val_acc: 0.3759 - val_top_k_categorical_accuracy: 0.9345\n",
      "Epoch 76/100\n",
      " - 18s - loss: 1.7186 - acc: 0.3741 - top_k_categorical_accuracy: 0.8691 - val_loss: 1.6319 - val_acc: 0.3793 - val_top_k_categorical_accuracy: 0.9310\n",
      "Epoch 77/100\n",
      " - 18s - loss: 1.6951 - acc: 0.3784 - top_k_categorical_accuracy: 0.8563 - val_loss: 1.6338 - val_acc: 0.3690 - val_top_k_categorical_accuracy: 0.9276\n",
      "Epoch 78/100\n",
      " - 18s - loss: 1.7241 - acc: 0.3940 - top_k_categorical_accuracy: 0.8578 - val_loss: 1.6241 - val_acc: 0.3759 - val_top_k_categorical_accuracy: 0.9345\n",
      "Epoch 79/100\n",
      " - 18s - loss: 1.7304 - acc: 0.3528 - top_k_categorical_accuracy: 0.8549 - val_loss: 1.6306 - val_acc: 0.3655 - val_top_k_categorical_accuracy: 0.9241\n",
      "Epoch 80/100\n",
      " - 18s - loss: 1.7264 - acc: 0.3812 - top_k_categorical_accuracy: 0.8620 - val_loss: 1.6337 - val_acc: 0.3724 - val_top_k_categorical_accuracy: 0.9276\n",
      "Epoch 81/100\n",
      " - 18s - loss: 1.7239 - acc: 0.3883 - top_k_categorical_accuracy: 0.8649 - val_loss: 1.6230 - val_acc: 0.3690 - val_top_k_categorical_accuracy: 0.9276\n",
      "Epoch 82/100\n",
      " - 18s - loss: 1.6727 - acc: 0.3997 - top_k_categorical_accuracy: 0.8819 - val_loss: 1.6183 - val_acc: 0.3724 - val_top_k_categorical_accuracy: 0.9276\n",
      "Epoch 83/100\n",
      " - 18s - loss: 1.7007 - acc: 0.3755 - top_k_categorical_accuracy: 0.8549 - val_loss: 1.6276 - val_acc: 0.3621 - val_top_k_categorical_accuracy: 0.9345\n",
      "Epoch 84/100\n",
      " - 18s - loss: 1.6820 - acc: 0.4225 - top_k_categorical_accuracy: 0.8762 - val_loss: 1.6313 - val_acc: 0.3724 - val_top_k_categorical_accuracy: 0.9241\n",
      "Epoch 85/100\n",
      " - 18s - loss: 1.6834 - acc: 0.3898 - top_k_categorical_accuracy: 0.8535 - val_loss: 1.6457 - val_acc: 0.3793 - val_top_k_categorical_accuracy: 0.9310\n",
      "Epoch 86/100\n",
      " - 18s - loss: 1.6630 - acc: 0.3869 - top_k_categorical_accuracy: 0.8848 - val_loss: 1.6425 - val_acc: 0.3621 - val_top_k_categorical_accuracy: 0.9345\n",
      "Epoch 87/100\n",
      " - 18s - loss: 1.6131 - acc: 0.4282 - top_k_categorical_accuracy: 0.8962 - val_loss: 1.6362 - val_acc: 0.3862 - val_top_k_categorical_accuracy: 0.9276\n",
      "Epoch 88/100\n",
      " - 18s - loss: 1.6647 - acc: 0.3940 - top_k_categorical_accuracy: 0.8663 - val_loss: 1.6406 - val_acc: 0.3862 - val_top_k_categorical_accuracy: 0.9345\n",
      "Epoch 89/100\n",
      " - 18s - loss: 1.6204 - acc: 0.4438 - top_k_categorical_accuracy: 0.8947 - val_loss: 1.6508 - val_acc: 0.4103 - val_top_k_categorical_accuracy: 0.9276\n",
      "Epoch 90/100\n",
      " - 18s - loss: 1.6246 - acc: 0.4438 - top_k_categorical_accuracy: 0.8933 - val_loss: 1.6635 - val_acc: 0.3793 - val_top_k_categorical_accuracy: 0.9276\n",
      "Epoch 91/100\n",
      " - 18s - loss: 1.6535 - acc: 0.3997 - top_k_categorical_accuracy: 0.8834 - val_loss: 1.6441 - val_acc: 0.3690 - val_top_k_categorical_accuracy: 0.9379\n",
      "Epoch 92/100\n",
      " - 18s - loss: 1.6582 - acc: 0.3969 - top_k_categorical_accuracy: 0.8819 - val_loss: 1.6544 - val_acc: 0.3828 - val_top_k_categorical_accuracy: 0.9414\n",
      "Epoch 93/100\n",
      " - 18s - loss: 1.6107 - acc: 0.4196 - top_k_categorical_accuracy: 0.8876 - val_loss: 1.6372 - val_acc: 0.3724 - val_top_k_categorical_accuracy: 0.9345\n",
      "Epoch 94/100\n",
      " - 18s - loss: 1.5898 - acc: 0.4509 - top_k_categorical_accuracy: 0.8919 - val_loss: 1.6276 - val_acc: 0.3690 - val_top_k_categorical_accuracy: 0.9379\n",
      "Epoch 95/100\n",
      " - 18s - loss: 1.6146 - acc: 0.4211 - top_k_categorical_accuracy: 0.8890 - val_loss: 1.6413 - val_acc: 0.3759 - val_top_k_categorical_accuracy: 0.9345\n",
      "Epoch 96/100\n",
      " - 18s - loss: 1.5635 - acc: 0.4424 - top_k_categorical_accuracy: 0.8805 - val_loss: 1.6682 - val_acc: 0.3793 - val_top_k_categorical_accuracy: 0.9448\n",
      "Epoch 97/100\n",
      " - 18s - loss: 1.6054 - acc: 0.4125 - top_k_categorical_accuracy: 0.8976 - val_loss: 1.6480 - val_acc: 0.3690 - val_top_k_categorical_accuracy: 0.9414\n",
      "Epoch 98/100\n",
      " - 18s - loss: 1.5848 - acc: 0.4381 - top_k_categorical_accuracy: 0.8947 - val_loss: 1.6455 - val_acc: 0.3759 - val_top_k_categorical_accuracy: 0.9414\n",
      "Epoch 99/100\n",
      " - 18s - loss: 1.5959 - acc: 0.4353 - top_k_categorical_accuracy: 0.8905 - val_loss: 1.6437 - val_acc: 0.3759 - val_top_k_categorical_accuracy: 0.9483\n",
      "Epoch 100/100\n",
      " - 18s - loss: 1.6024 - acc: 0.4182 - top_k_categorical_accuracy: 0.8962 - val_loss: 1.6456 - val_acc: 0.3724 - val_top_k_categorical_accuracy: 0.9448\n"
     ]
    }
   ],
   "source": [
    "x_train = HDF5Matrix('train_8.h5', 'train')\n",
    "y_train = HDF5Matrix('train_8.h5', 'train_labels')\n",
    "x_test = HDF5Matrix('test_8.h5', 'test')\n",
    "y_test = HDF5Matrix('test_8.h5', 'test_labels')\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train[240])\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "    \n",
    "model, callback_list = lstm()\n",
    "#model.fit(x_train, y_train)\n",
    "model.fit(x_train, y_train, batch_size = BATCH_SIZE, epochs = 100,verbose = 2,validation_data = (x_test, y_test),shuffle = 'batch', callbacks=callback_list)\n",
    "    \n",
    "model.save(\"Activity_Recognition.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 907
    },
    "colab_type": "code",
    "id": "adXVxAIHCf7O",
    "outputId": "83a5c857-558b-4e19-a9c2-5a48ee64f81d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Activity_Recognition.h5\t   'models\\checkpoint-50-1.71.hdf5'\n",
      " Human-Action-Classification-\t   'models\\checkpoint-51-1.71.hdf5'\n",
      " logs\t\t\t\t   'models\\checkpoint-52-1.70.hdf5'\n",
      "'models\\checkpoint-01-2.24.hdf5'   'models\\checkpoint-53-1.71.hdf5'\n",
      "'models\\checkpoint-02-2.20.hdf5'   'models\\checkpoint-54-1.70.hdf5'\n",
      "'models\\checkpoint-03-2.16.hdf5'   'models\\checkpoint-55-1.70.hdf5'\n",
      "'models\\checkpoint-04-2.14.hdf5'   'models\\checkpoint-56-1.70.hdf5'\n",
      "'models\\checkpoint-05-2.11.hdf5'   'models\\checkpoint-57-1.68.hdf5'\n",
      "'models\\checkpoint-06-2.07.hdf5'   'models\\checkpoint-58-1.69.hdf5'\n",
      "'models\\checkpoint-07-2.06.hdf5'   'models\\checkpoint-59-1.68.hdf5'\n",
      "'models\\checkpoint-08-2.05.hdf5'   'models\\checkpoint-60-1.70.hdf5'\n",
      "'models\\checkpoint-09-2.04.hdf5'   'models\\checkpoint-61-1.69.hdf5'\n",
      "'models\\checkpoint-100-1.65.hdf5'  'models\\checkpoint-62-1.69.hdf5'\n",
      "'models\\checkpoint-10-2.03.hdf5'   'models\\checkpoint-63-1.70.hdf5'\n",
      "'models\\checkpoint-11-2.02.hdf5'   'models\\checkpoint-64-1.69.hdf5'\n",
      "'models\\checkpoint-12-2.00.hdf5'   'models\\checkpoint-65-1.69.hdf5'\n",
      "'models\\checkpoint-13-1.99.hdf5'   'models\\checkpoint-66-1.68.hdf5'\n",
      "'models\\checkpoint-14-1.98.hdf5'   'models\\checkpoint-67-1.68.hdf5'\n",
      "'models\\checkpoint-15-1.97.hdf5'   'models\\checkpoint-68-1.67.hdf5'\n",
      "'models\\checkpoint-16-1.96.hdf5'   'models\\checkpoint-69-1.68.hdf5'\n",
      "'models\\checkpoint-17-1.95.hdf5'   'models\\checkpoint-70-1.65.hdf5'\n",
      "'models\\checkpoint-18-1.94.hdf5'   'models\\checkpoint-71-1.65.hdf5'\n",
      "'models\\checkpoint-19-1.92.hdf5'   'models\\checkpoint-72-1.65.hdf5'\n",
      "'models\\checkpoint-20-1.91.hdf5'   'models\\checkpoint-73-1.63.hdf5'\n",
      "'models\\checkpoint-21-1.91.hdf5'   'models\\checkpoint-74-1.64.hdf5'\n",
      "'models\\checkpoint-22-1.90.hdf5'   'models\\checkpoint-75-1.64.hdf5'\n",
      "'models\\checkpoint-23-1.89.hdf5'   'models\\checkpoint-76-1.63.hdf5'\n",
      "'models\\checkpoint-24-1.88.hdf5'   'models\\checkpoint-77-1.63.hdf5'\n",
      "'models\\checkpoint-25-1.87.hdf5'   'models\\checkpoint-78-1.62.hdf5'\n",
      "'models\\checkpoint-26-1.87.hdf5'   'models\\checkpoint-79-1.63.hdf5'\n",
      "'models\\checkpoint-27-1.87.hdf5'   'models\\checkpoint-80-1.63.hdf5'\n",
      "'models\\checkpoint-28-1.85.hdf5'   'models\\checkpoint-81-1.62.hdf5'\n",
      "'models\\checkpoint-29-1.85.hdf5'   'models\\checkpoint-82-1.62.hdf5'\n",
      "'models\\checkpoint-30-1.84.hdf5'   'models\\checkpoint-83-1.63.hdf5'\n",
      "'models\\checkpoint-31-1.84.hdf5'   'models\\checkpoint-84-1.63.hdf5'\n",
      "'models\\checkpoint-32-1.83.hdf5'   'models\\checkpoint-85-1.65.hdf5'\n",
      "'models\\checkpoint-33-1.83.hdf5'   'models\\checkpoint-86-1.64.hdf5'\n",
      "'models\\checkpoint-34-1.82.hdf5'   'models\\checkpoint-87-1.64.hdf5'\n",
      "'models\\checkpoint-35-1.82.hdf5'   'models\\checkpoint-88-1.64.hdf5'\n",
      "'models\\checkpoint-36-1.81.hdf5'   'models\\checkpoint-89-1.65.hdf5'\n",
      "'models\\checkpoint-37-1.81.hdf5'   'models\\checkpoint-90-1.66.hdf5'\n",
      "'models\\checkpoint-38-1.80.hdf5'   'models\\checkpoint-91-1.64.hdf5'\n",
      "'models\\checkpoint-39-1.79.hdf5'   'models\\checkpoint-92-1.65.hdf5'\n",
      "'models\\checkpoint-40-1.79.hdf5'   'models\\checkpoint-93-1.64.hdf5'\n",
      "'models\\checkpoint-41-1.78.hdf5'   'models\\checkpoint-94-1.63.hdf5'\n",
      "'models\\checkpoint-42-1.77.hdf5'   'models\\checkpoint-95-1.64.hdf5'\n",
      "'models\\checkpoint-43-1.77.hdf5'   'models\\checkpoint-96-1.67.hdf5'\n",
      "'models\\checkpoint-44-1.77.hdf5'   'models\\checkpoint-97-1.65.hdf5'\n",
      "'models\\checkpoint-45-1.74.hdf5'   'models\\checkpoint-98-1.65.hdf5'\n",
      "'models\\checkpoint-46-1.75.hdf5'   'models\\checkpoint-99-1.64.hdf5'\n",
      "'models\\checkpoint-47-1.76.hdf5'    sample_data\n",
      "'models\\checkpoint-48-1.73.hdf5'    test_8.h5\n",
      "'models\\checkpoint-49-1.73.hdf5'    train_8.h5\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Activity Recognition using RNNs.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
